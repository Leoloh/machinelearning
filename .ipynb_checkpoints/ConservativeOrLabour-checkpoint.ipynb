{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "import argparse\n",
    "import os, sys\n",
    "from six.moves import xrange\n",
    "\n",
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP:\n",
    "    def __init__(self, name, twitter_handle, party):\n",
    "        self.name = name;\n",
    "        self.twitter_handle = twitter_handle\n",
    "        self.party = party\n",
    "        self.tweets = []\n",
    "    def addTweet(self, tweet):\n",
    "        self.tweets.append(tweet)\n",
    "    def __str__(self):\n",
    "        return 'Name: %s, TW: %s, Party: %s' % (self.name, self.twitter_handle, self.party)\n",
    "        \n",
    "list_of_mps = []\n",
    "\n",
    "url = 'https://www.mpsontwitter.co.uk/list'\n",
    "with urllib.request.urlopen(url) as doc:\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    rows = soup.find_all('tr')\n",
    "    for i in range(2,len(rows)):\n",
    "        party = rows[i].find('td')['class'][0]\n",
    "        twitter_handle = rows[i].find('a').getText()\n",
    "        name = rows[i].find_all('td')[2].getText()\n",
    "        list_of_mps.append(MP(name, twitter_handle, party))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweepy API\n",
    "Using the [Tweepy API](http://tweepy.readthedocs.io/en/v3.5.0/) to extract the tweets from each of the MPs - I have already used pickle to export these into a txt file so I no longer need to run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n"
     ]
    }
   ],
   "source": [
    "consumer_key = 'your_key'\n",
    "consumer_secret = 'you_secret'\n",
    "access_token = 'your_token'\n",
    "access_secret = 'your_access_secret'\n",
    "\n",
    "\n",
    "import tweepy as tw\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tw.API(auth)\n",
    "\n",
    "for mp in list_of_mps:\n",
    "    try:\n",
    "        tweets = api.user_timeline(mp.twitter_handle, count=50)\n",
    "        for tweet in tweets:\n",
    "            mp.addTweet(tweet.text)\n",
    "    except tw.TweepError:\n",
    "        print('Protected Tweets :(')\n",
    "        \n",
    "with open('test.txt', 'wb') as testFile:\n",
    "    pickle.dump(list_of_mps, testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Using the nltk package we can tokenize the tweets into each twitter token - this can be useful because it can deal with links, @twitter_users etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import math\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "with open('data.txt', 'rb') as testFile:\n",
    "    mps = pickle.load(testFile)\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "vocabulary_size = 50000\n",
    "\n",
    "for mp in mps:\n",
    "    if len(mp.tweets) > 0:\n",
    "        for tweet in mp.tweets:\n",
    "            tokens = tknzr.tokenize(tweet)\n",
    "            for token in tokens:\n",
    "                vocabulary.append(token)\n",
    "            \n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = {}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    unk_count = 0\n",
    "    data = []\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "        \n",
    "    \n",
    "data, count, dictionary, reversed_dictionary = build_dataset(vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 to -> 206 @Conservatives\n",
      "3 to -> 1754 committed\n",
      "3 to -> 10 a\n",
      "3 to -> 9 and\n",
      "10 a -> 206 @Conservatives\n",
      "10 a -> 30 are\n",
      "10 a -> 1754 committed\n",
      "10 a -> 914 #NHS\n",
      "3722 comprehensive -> 9 and\n",
      "3722 comprehensive -> 914 #NHS\n",
      "3722 comprehensive -> 1754 committed\n",
      "3722 comprehensive -> 5 .\n",
      "9 and -> 2432 universal\n",
      "9 and -> 3 to\n",
      "9 and -> 3722 comprehensive\n",
      "9 and -> 1754 committed\n",
      "2432 universal -> 3 to\n",
      "2432 universal -> 5 .\n",
      "2432 universal -> 914 #NHS\n",
      "2432 universal -> 3722 comprehensive\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def batch_generator(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = batch_generator(batch_size=20, num_skips=4, skip_window=4)\n",
    "\n",
    "for i in range(20):\n",
    "    print(batch[i], reversed_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reversed_dictionary[labels[i, 0]])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n",
    "Now it is time to start using tensorflow and the word2vec model - they have a great [tutorial](https://www.tensorflow.org/tutorials/representation/word2vec) on their website, but let's go over a few basic concepts the first being 'What is an embedding?'. \n",
    "\n",
    "An embedding is a way to take our input data (the word strings) and convert them to numbers. Neural networks generally work in numbers in order to do all of the calculations like matrix multiplication, gradient-descent etc. Tensorflow, lucky for us, has a built-in function that can create for us an embedding of a desired shape. For us we want a matrix of vocabulary_size (all the unique words in our data) and we can give an arbitary embedding_size. \n",
    "\n",
    "We will use the Noise Contrastive Estimation loss as our 'how well did we do?' function - this essentially draws on randomly selected other target words and compares them to our guess. When our guess has a high probability and the random selection of 'noisy' words has a low probability, we are doing well. This reduces the computation from testing all V words in our vocabulary to a pre-selected K noisy words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant Variables\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "num_sampled = 64\n",
    "\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "            \n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "            \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "            \n",
    "    tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "            \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings/norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        \n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mergerd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-97941e9d4dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrun_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         _, summary, loss_val = session.run(\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmergerd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             run_metadata=run_metadata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mergerd' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "    init.run()\n",
    "    print('INIT')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = batch_generator(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        run_metadata = tf.RunMetadata()\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        writer.add_summary(summary, step)\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "            \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
