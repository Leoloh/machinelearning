{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP:\n",
    "    def __init__(self, name, twitter_handle, party):\n",
    "        self.name = name;\n",
    "        self.twitter_handle = twitter_handle\n",
    "        self.party = party\n",
    "        self.tweets = []\n",
    "    def addTweet(self, tweet):\n",
    "        self.tweets.append(tweet)\n",
    "    def __str__(self):\n",
    "        return 'Name: %s, TW: %s, Party: %s' % (self.name, self.twitter_handle, self.party)\n",
    "        \n",
    "list_of_mps = []\n",
    "\n",
    "url = 'https://www.mpsontwitter.co.uk/list'\n",
    "with urllib.request.urlopen(url) as doc:\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    rows = soup.find_all('tr')\n",
    "    for i in range(2,len(rows)):\n",
    "        party = rows[i].find('td')['class'][0]\n",
    "        twitter_handle = rows[i].find('a').getText()\n",
    "        name = rows[i].find_all('td')[2].getText()\n",
    "        list_of_mps.append(MP(name, twitter_handle, party))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n",
      "Protected Tweets :(\n"
     ]
    }
   ],
   "source": [
    "consumer_key = 'MVb3kWys0DQXRBmcWWkyE7Gbz'\n",
    "consumer_secret = 'xUr49m74r9wNYWu0JJWMvfMQ0sDCrISaFyjJlPqu0JS2J7hRmf'\n",
    "access_token = '1239937231-A3hMA2n7CWtQjdozut3bUqhUcNMK9L1FvGepLX1'\n",
    "access_secret = 'lDETflnQ8khU4iIisPaixcVNe0a7dLKKwVgWNDBzhvFEe'\n",
    "\n",
    "\n",
    "import tweepy as tw\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tw.API(auth)\n",
    "\n",
    "for mp in list_of_mps:\n",
    "    try:\n",
    "        tweets = api.user_timeline(mp.twitter_handle, count=50)\n",
    "        for tweet in tweets:\n",
    "            mp.addTweet(tweet.text)\n",
    "    except tw.TweepError:\n",
    "        print('Protected Tweets :(')\n",
    "        \n",
    "with open('test.txt', 'wb') as testFile:\n",
    "    pickle.dump(list_of_mps, testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import math\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "with open('data.txt', 'rb') as testFile:\n",
    "    mps = pickle.load(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'the quick brown fox jumped over the lazy dog'\n",
    "t = test.split()\n",
    "def skip_gram(dataset, vocabulary, window_size=2):\n",
    "    pairs = []\n",
    "    for (idx, word) in enumerate(dataset):\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 1\n",
    "        if idx >= window_size and idx < len(dataset)-window_size:\n",
    "            for i in range(idx - window_size, idx + window_size + 1):\n",
    "                if idx != i:\n",
    "                    pairs.append((dataset[idx], dataset[i]))\n",
    "    return pairs\n",
    "\n",
    "dataset = []\n",
    "vocabulary = {}\n",
    "\n",
    "for mp in mps:\n",
    "    if len(mp.tweets) > 0:\n",
    "        mp_skip_gram = []\n",
    "        for tweet in mp.tweets:\n",
    "            tweet_list = tknzr.tokenize(tweet)\n",
    "            mp_skip_gram += skip_gram(tweet_list, vocabulary)\n",
    "        dataset += mp_skip_gram\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "dataset_size = len(dataset)\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0/math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
